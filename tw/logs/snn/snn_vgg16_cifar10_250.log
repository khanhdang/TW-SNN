
 Run on time: 2021-07-12 16:03:49.698650

 Arguments: 
	 gpu                  : True
	 seed                 : 0
	 dataset              : CIFAR10
	 batch_size           : 64
	 architecture         : VGG16
	 learning_rate        : 0.0001
	 pretrained_ann       : ./trained_models/ann/ann_vgg16_cifar10.pth
	 pretrained_snn       : 
	 en_find_threshold    : False
	 test_only            : True
	 log                  : True
	 epochs               : 2
	 lr_interval          : [1, 1, 1]
	 lr_reduce            : 5
	 timesteps            : 250
	 leak                 : 1.0
	 scaling_factor       : 0.7
	 default_threshold    : 1.0
	 activation           : Linear
	 alpha                : 0.3
	 beta                 : 0.01
	 optimizer            : Adam
	 weight_decay         : 0.0005
	 momentum             : 0.95
	 amsgrad              : True
	 dropout              : 0.3
	 kernel_size          : 3
	 test_acc_every_batch : False
	 train_acc_batches    : 200
	 devices              : 0
 Success: Loaded module.features.0.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.3.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.6.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.9.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.12.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.15.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.18.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.21.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.24.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.27.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.30.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.33.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.features.36.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.classifier.0.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.classifier.3.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Success: Loaded module.classifier.6.weight from ./trained_models/ann/ann_vgg16_cifar10.pth
 Info: Accuracy of loaded ANN model: 0.9037
 Info: Thresholds loaded from trained ANN: [8.396539688110352, 10.356905937194824, 1.4366652965545654, 1.7861534357070923, 0.3077472150325775, 0.8923217058181763, 0.7281284332275391, 0.2292354851961136, 1.2855751514434814, 1.2652335166931152, 0.47809961438179016, 0.8992334604263306, 1.042016863822937, 1.318678855895996, 1.2510868310928345]
 DataParallel(
  (module): VGG_SNN_STDB(
    (input_layer): PoissonGenerator()
    (features): Sequential(
      (0): quanConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): ReLU(inplace)
      (2): Dropout(p=0.3)
      (3): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): ReLU(inplace)
      (5): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (6): quanConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): ReLU(inplace)
      (8): Dropout(p=0.3)
      (9): quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): ReLU(inplace)
      (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (12): quanConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): ReLU(inplace)
      (14): Dropout(p=0.3)
      (15): quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): ReLU(inplace)
      (17): Dropout(p=0.3)
      (18): quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): ReLU(inplace)
      (20): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (21): quanConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): ReLU(inplace)
      (23): Dropout(p=0.3)
      (24): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): ReLU(inplace)
      (26): Dropout(p=0.3)
      (27): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): ReLU(inplace)
      (29): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (30): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): ReLU(inplace)
      (32): Dropout(p=0.3)
      (33): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): ReLU(inplace)
      (35): Dropout(p=0.3)
      (36): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): ReLU(inplace)
      (38): Dropout(p=0.3)
    )
    (classifier): Sequential(
      (0): quanLinear(in_features=2048, out_features=4096, bias=False)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): quanLinear(in_features=4096, out_features=4096, bias=False)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): quanLinear(in_features=4096, out_features=10, bias=False)
    )
  )
)
 Adam (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0005
) test_loss: 197.7144, test_acc: 0.8871, best: 0.8871 time: 3:24:11
 Highest accuracy: 0.8971